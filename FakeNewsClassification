{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6641062,"sourceType":"datasetVersion","datasetId":2093157}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!python3 -m venv venv\n!source venv/bin/activate\n!pip install tensorflow transformers\n\n!pip install imbalanced-learn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-23T04:15:43.330499Z","iopub.execute_input":"2024-10-23T04:15:43.331264Z","iopub.status.idle":"2024-10-23T04:16:14.861964Z","shell.execute_reply.started":"2024-10-23T04:15:43.331209Z","shell.execute_reply":"2024-10-23T04:16:14.860890Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom transformers import TFDistilBertModel, DistilBertTokenizer, TFDistilBertForSequenceClassification\nimport pickle\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\n\nfrom tensorflow.keras.layers import Dropout, Input, Dense, Lambda\nfrom tensorflow.keras.models import Model\n\nimport os\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\n\nfrom transformers import logging","metadata":{"execution":{"iopub.status.busy":"2024-10-23T04:16:14.864333Z","iopub.execute_input":"2024-10-23T04:16:14.864679Z","iopub.status.idle":"2024-10-23T04:16:38.948453Z","shell.execute_reply.started":"2024-10-23T04:16:14.864638Z","shell.execute_reply":"2024-10-23T04:16:38.947543Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/fake-news-classification/WELFake_Dataset.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-23T04:16:38.949733Z","iopub.execute_input":"2024-10-23T04:16:38.950282Z","iopub.status.idle":"2024-10-23T04:16:44.433538Z","shell.execute_reply.started":"2024-10-23T04:16:38.950247Z","shell.execute_reply":"2024-10-23T04:16:44.432259Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                              title  \\\n0           0  LAW ENFORCEMENT ON HIGH ALERT Following Threat...   \n1           1                                                NaN   \n2           2  UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...   \n3           3  Bobby Jindal, raised Hindu, uses story of Chri...   \n4           4  SATAN 2: Russia unvelis an image of its terrif...   \n\n                                                text  label  \n0  No comment is expected from Barack Obama Membe...      1  \n1     Did they post their votes for Hillary already?      1  \n2   Now, most of the demonstrators gathered last ...      1  \n3  A dozen politically active pastors came here f...      0  \n4  The RS-28 Sarmat missile, dubbed Satan 2, will...      1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>title</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>LAW ENFORCEMENT ON HIGH ALERT Following Threat...</td>\n      <td>No comment is expected from Barack Obama Membe...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>Did they post their votes for Hillary already?</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...</td>\n      <td>Now, most of the demonstrators gathered last ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Bobby Jindal, raised Hindu, uses story of Chri...</td>\n      <td>A dozen politically active pastors came here f...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>SATAN 2: Russia unvelis an image of its terrif...</td>\n      <td>The RS-28 Sarmat missile, dubbed Satan 2, will...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-10-23T04:16:44.435351Z","iopub.execute_input":"2024-10-23T04:16:44.435851Z","iopub.status.idle":"2024-10-23T04:16:44.458935Z","shell.execute_reply.started":"2024-10-23T04:16:44.435795Z","shell.execute_reply":"2024-10-23T04:16:44.457124Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"label\n1    37106\n0    35028\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"x = list(df['text'])\ny = list(df['label'])","metadata":{"execution":{"iopub.status.busy":"2024-10-23T04:16:44.463627Z","iopub.execute_input":"2024-10-23T04:16:44.464106Z","iopub.status.idle":"2024-10-23T04:16:44.492158Z","shell.execute_reply.started":"2024-10-23T04:16:44.464054Z","shell.execute_reply":"2024-10-23T04:16:44.491154Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"MODEL_NAME = 'distilbert-base-uncased'\nMAX_LEN = 100\n\nreview = x[0]\n\n# Initialize tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n\ninputs = tokenizer(review, max_length=MAX_LEN, truncation=True, padding=True, clean_up_tokenization_spaces=True)\n\n# print(f'review: \\'{review}\\'')\n# print(f'input ids: {inputs[\"input_ids\"]}')\n# print(f'attention mask: {inputs[\"attention_mask\"]}')","metadata":{"execution":{"iopub.status.busy":"2024-10-23T04:16:44.493412Z","iopub.execute_input":"2024-10-23T04:16:44.493821Z","iopub.status.idle":"2024-10-23T04:16:45.601024Z","shell.execute_reply.started":"2024-10-23T04:16:44.493777Z","shell.execute_reply":"2024-10-23T04:16:45.600095Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"518fb0a571a645c69455fcdbd7294d76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"724cff50b90e4aaf8285f7c3449356de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f8ef11428524c13bac07281eb19bf3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a331f0d0afc24969896ffa27fad80ac7"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nKeyword arguments {'clean_up_tokenization_spaces': True} not recognized.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define Model","metadata":{}},{"cell_type":"code","source":"# Define helper functions for encoding and constructing the dataset\ndef construct_encodings(x, tokenizer, max_len, trucation=True, padding=True):\n    return tokenizer(x, max_length=max_len, truncation=trucation, padding=padding)","metadata":{"execution":{"iopub.status.busy":"2024-10-23T04:16:45.602466Z","iopub.execute_input":"2024-10-23T04:16:45.603460Z","iopub.status.idle":"2024-10-23T04:16:45.609140Z","shell.execute_reply.started":"2024-10-23T04:16:45.603404Z","shell.execute_reply":"2024-10-23T04:16:45.607952Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"x = [str(item) for item in x]","metadata":{"execution":{"iopub.status.busy":"2024-10-23T04:16:45.610579Z","iopub.execute_input":"2024-10-23T04:16:45.610950Z","iopub.status.idle":"2024-10-23T04:16:45.642800Z","shell.execute_reply.started":"2024-10-23T04:16:45.610913Z","shell.execute_reply":"2024-10-23T04:16:45.641735Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"encodings = construct_encodings(x, tokenizer, max_len=MAX_LEN)\n#below: SMOTE\nTEST_SPLIT = 0.3  # Define the test split ratio\nBATCH_SIZE = 8\n\ninput_ids = encodings['input_ids']\nattention_masks = encodings['attention_mask']\n\nx_train, x_test, y_train, y_test, masks_train, masks_test = train_test_split(\n    input_ids, y, attention_masks, test_size=TEST_SPLIT, random_state=42, stratify=y\n)\n\n# Apply SMOTE to the training data only\nsmote = SMOTE(random_state=42)\nx_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)\n\n# Adjust attention masks for the new samples generated by SMOTE\n# Note: SMOTE only generates synthetic samples based on input features. We'll copy masks for simplicity.\nmasks_train_resampled = np.repeat(masks_train, np.ceil(len(x_train_resampled) / len(masks_train)).astype(int), axis=0)[:len(x_train_resampled)]\n\n# Reconstruct the encodings with resampled data\ntrain_encodings = {'input_ids': x_train_resampled, 'attention_mask': masks_train_resampled}\ntest_encodings = {'input_ids': x_test, 'attention_mask': masks_test}\n","metadata":{"execution":{"iopub.status.busy":"2024-10-23T04:16:45.644190Z","iopub.execute_input":"2024-10-23T04:16:45.644576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Changes:** I used SMOTE to create a more balanced training dataset.","metadata":{}},{"cell_type":"code","source":"def construct_tfdataset(encodings, y=None):\n    if y is not None:\n        return tf.data.Dataset.from_tensor_slices((dict(encodings), y))\n    else:\n        return tf.data.Dataset.from_tensor_slices(dict(encodings))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Changes:** I updated construct_tfdataset function.(SMOTE)\n\n","metadata":{}},{"cell_type":"code","source":"# Create TensorFlow datasets\n# SMOTE\n\ntfdataset_train = construct_tfdataset(train_encodings, y_train_resampled).batch(BATCH_SIZE)\ntfdataset_test = construct_tfdataset(test_encodings, y_test).batch(BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train and evaluate","metadata":{}},{"cell_type":"code","source":"# Suppress transformers model loading warnings\nlogging.set_verbosity_error()\n\n# Define model parameters\nN_EPOCHS = 5\nDROPOUT_RATE = 0.3\n\n# Load the pre-trained DistilBERT base model (without classification head)\nbase_model = TFDistilBertModel.from_pretrained(MODEL_NAME)\n\n# Define input layers\ninput_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\nattention_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"attention_mask\")\n\n# Wrap the base model call in a Lambda layer to ensure compatibility\ndistilbert_output = Lambda(\n    lambda x: base_model(x[0], attention_mask=x[1])[0],  # Extract the last hidden state directly\n    output_shape=(MAX_LEN, base_model.config.hidden_size)\n)([input_ids, attention_mask])\n\n# Extract the [CLS] token's hidden state\nhidden_state = distilbert_output[:, 0, :]  # Extract the first token ([CLS]) representation\n\n# Apply dropout with a higher rate (e.g., 0.3 or 0.5)\ndropout = Dropout(DROPOUT_RATE)(hidden_state)\n\n# Output layer for classification (binary classification, so one Dense layer with 2 units)\noutput = Dense(2, activation='softmax')(dropout)\n\n# Create the new model\nmodel = Model(inputs=[input_ids, attention_mask], outputs=output)\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, clipnorm=1.0)\n# Compile model with the optimizer\nmodel.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n\n# 5. Print model summary to see the updated architecture\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Changes:** I added Gradient Clipping(clipnorm=1.0) and increased dropout rate to resolve overfitting\n\n* Gradient clipping ensures that updates to model weights don’t become too large, maintaining stability during training.\n* Track F1-score and precision/recall to ensure balanced performance. The classification report shows data imbalance so added this to monitor.\n* I added a Dropout layer with a rate of 0.3 after the DistilBERT output. This change aims to reduce overfitting by randomly deactivating 30% of neurons during training, which can help the model generalize better to new data.","metadata":{}},{"cell_type":"code","source":"# Calculate class weights\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(y),\n    y=y\n)\nclass_weights_dict = dict(enumerate(class_weights))\nprint(\"Class Weights:\", class_weights_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Changes:** I added class weights to address data imbalance.\n\n* Class weights help to give more importance to the minority class during training, which balances out the learning process.\n* This method is suitable because it integrates well with the model.fit() method of Keras without needing to change the structure of the dataset.","metadata":{}},{"cell_type":"code","source":"# Train the model with the callbacks\nhistory = model.fit(\n    tfdataset_train,\n    validation_data=tfdataset_test,  # Provide validation data to monitor val_loss\n    epochs=N_EPOCHS,\n    validation_freq=1,\n    class_weight=class_weights_dict,\n)\n# Set TensorFlow log level to suppress warnings and info messages\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" **Changes:** I added validation_freq=1\n* By default, model.fit() validates at the end of each epoch, but you can adjust this to validate more frequently to keep a closer eye on validation performance.\n","metadata":{}},{"cell_type":"code","source":"benchmarks = model.evaluate(tfdataset_test, return_dict=True, batch_size=BATCH_SIZE)\nprint(benchmarks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_predictor(model, model_name, max_len):\n  tkzr = DistilBertTokenizer.from_pretrained(model_name)\n  def predict_proba(text):\n      x = [text]\n\n      # Explicitly set clean_up_tokenization_spaces to avoid the warning\n      encodings = construct_encodings(x, tkzr, max_len=max_len, padding=True, truncation=True, clean_up_tokenization_spaces=True)\n      tfdataset = construct_tfdataset(encodings)\n      tfdataset = tfdataset.batch(1)\n\n      preds = model.predict(tfdataset).logits\n      preds = tf.nn.softmax(tf.convert_to_tensor(preds)).numpy() # Using tf.nn.softmax for clarity\n      return preds[0][0]\n    \n  return predict_proba\n\nclf = create_predictor(model, MODEL_NAME, MAX_LEN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The code below is what I added since the original code didn’t include any visualization. I referred to the second reference code (BERT model) since it had a lot of visualizations. I’ve added things like the confusion matrix and classification report.**","metadata":{}},{"cell_type":"markdown","source":"# Data Analysis","metadata":{}},{"cell_type":"code","source":"# 1. Plot Training/Validation Loss and Accuracy\ndef plot_metrics(history):\n    plt.figure(figsize=(12, 5))\n    # Plot accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.legend()\n\n    # Plot loss\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n\n    plt.show()\n\nplot_metrics(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Suppress TensorFlow warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Set to '2' for warnings only, '3' to suppress info & warnings\n\n# 2. Evaluate and Generate Predictions\ny_pred_probs = model.predict(tfdataset_test)\ny_pred = tf.argmax(y_pred_probs, axis=1).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get true labels\ny_true = []\nfor _, labels in tfdataset_test.unbatch():\n    y_true.append(labels.numpy())\ny_true = tf.convert_to_tensor(y_true).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3. Print Classification Report\nprint(\"Classification Report:\")\nprint(classification_report(y_true, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4. Plot Confusion Matrix\nconf_matrix = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ensure the directory exists for saving the model\nos.makedirs('./model', exist_ok=True)\n# Save the Keras model with a proper extension.\nmodel.save('./model/clf.keras') # Use .keras or .h5 as the file extension.\n# Save additional information like model name and MAX_LEN using pickle.\nwith open('./model/info.pkl', 'wb') as f:\n    pickle.dump((MODEL_NAME, MAX_LEN), f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}