{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6641062,"sourceType":"datasetVersion","datasetId":2093157}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!python3 -m venv venv\n!source venv/bin/activate\n!pip install tensorflow transformers\n\n!pip install imbalanced-learn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-20T04:15:41.955653Z","iopub.execute_input":"2024-10-20T04:15:41.956110Z","iopub.status.idle":"2024-10-20T04:16:09.457202Z","shell.execute_reply.started":"2024-10-20T04:15:41.956053Z","shell.execute_reply":"2024-10-20T04:16:09.455889Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom transformers import TFDistilBertModel, DistilBertTokenizer, TFDistilBertForSequenceClassification\nimport pickle\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\n\nfrom tensorflow.keras.layers import Dropout, Input, Dense, Lambda\nfrom tensorflow.keras.models import Model\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T04:16:09.459719Z","iopub.execute_input":"2024-10-20T04:16:09.460069Z","iopub.status.idle":"2024-10-20T04:16:09.466803Z","shell.execute_reply.started":"2024-10-20T04:16:09.460034Z","shell.execute_reply":"2024-10-20T04:16:09.465854Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/fake-news-classification/WELFake_Dataset.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-20T04:16:09.468018Z","iopub.execute_input":"2024-10-20T04:16:09.468352Z","iopub.status.idle":"2024-10-20T04:16:12.304343Z","shell.execute_reply.started":"2024-10-20T04:16:09.468319Z","shell.execute_reply":"2024-10-20T04:16:12.303371Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                              title  \\\n0           0  LAW ENFORCEMENT ON HIGH ALERT Following Threat...   \n1           1                                                NaN   \n2           2  UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...   \n3           3  Bobby Jindal, raised Hindu, uses story of Chri...   \n4           4  SATAN 2: Russia unvelis an image of its terrif...   \n\n                                                text  label  \n0  No comment is expected from Barack Obama Membe...      1  \n1     Did they post their votes for Hillary already?      1  \n2   Now, most of the demonstrators gathered last ...      1  \n3  A dozen politically active pastors came here f...      0  \n4  The RS-28 Sarmat missile, dubbed Satan 2, will...      1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>title</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>LAW ENFORCEMENT ON HIGH ALERT Following Threat...</td>\n      <td>No comment is expected from Barack Obama Membe...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>Did they post their votes for Hillary already?</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...</td>\n      <td>Now, most of the demonstrators gathered last ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Bobby Jindal, raised Hindu, uses story of Chri...</td>\n      <td>A dozen politically active pastors came here f...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>SATAN 2: Russia unvelis an image of its terrif...</td>\n      <td>The RS-28 Sarmat missile, dubbed Satan 2, will...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-10-20T04:16:12.306477Z","iopub.execute_input":"2024-10-20T04:16:12.306823Z","iopub.status.idle":"2024-10-20T04:16:12.315333Z","shell.execute_reply.started":"2024-10-20T04:16:12.306787Z","shell.execute_reply":"2024-10-20T04:16:12.314283Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"label\n1    37106\n0    35028\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"x = list(df['text'])\ny = list(df['label'])","metadata":{"execution":{"iopub.status.busy":"2024-10-20T04:16:12.316502Z","iopub.execute_input":"2024-10-20T04:16:12.316843Z","iopub.status.idle":"2024-10-20T04:16:12.406122Z","shell.execute_reply.started":"2024-10-20T04:16:12.316788Z","shell.execute_reply":"2024-10-20T04:16:12.405277Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"MODEL_NAME = 'distilbert-base-uncased'\nMAX_LEN = 100\n\nreview = x[0]\n\n# Initialize tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n\ninputs = tokenizer(review, max_length=MAX_LEN, truncation=True, padding=True)\n\nprint(f'review: \\'{review}\\'')\nprint(f'input ids: {inputs[\"input_ids\"]}')\nprint(f'attention mask: {inputs[\"attention_mask\"]}')","metadata":{"execution":{"iopub.status.busy":"2024-10-20T04:16:12.407156Z","iopub.execute_input":"2024-10-20T04:16:12.407462Z","iopub.status.idle":"2024-10-20T04:16:12.659848Z","shell.execute_reply.started":"2024-10-20T04:16:12.407430Z","shell.execute_reply":"2024-10-20T04:16:12.658779Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"review: 'No comment is expected from Barack Obama Members of the #FYF911 or #FukYoFlag and #BlackLivesMatter movements called for the lynching and hanging of white people and cops. They encouraged others on a radio show Tuesday night to  turn the tide  and kill white people and cops to send a message about the killing of black people in America.One of the F***YoFlag organizers is called  Sunshine.  She has a radio blog show hosted from Texas called,  Sunshine s F***ing Opinion Radio Show. A snapshot of her #FYF911 @LOLatWhiteFear Twitter page at 9:53 p.m. shows that she was urging supporters to  Call now!! #fyf911 tonight we continue to dismantle the illusion of white Below is a SNAPSHOT Twitter Radio Call Invite   #FYF911The radio show aired at 10:00 p.m. eastern standard time.During the show, callers clearly call for  lynching  and  killing  of white people.A 2:39 minute clip from the radio show can be heard here. It was provided to Breitbart Texas by someone who would like to be referred to as  Hannibal.  He has already received death threats as a result of interrupting #FYF911 conference calls.An unidentified black man said  when those mother f**kers are by themselves, that s when when we should start f***ing them up. Like they do us, when a bunch of them ni**ers takin  one of us out, that s how we should roll up.  He said,  Cause we already roll up in gangs anyway. There should be six or seven black mother f**ckers, see that white person, and then lynch their ass. Let s turn the tables. They conspired that if  cops started losing people,  then  there will be a state of emergency. He speculated that one of two things would happen,  a big-ass [R s?????] war,  or  ni**ers, they are going to start backin  up. We are already getting killed out here so what the f**k we got to lose? Sunshine could be heard saying,  Yep, that s true. That s so f**king true. He said,  We need to turn the tables on them. Our kids are getting shot out here. Somebody needs to become a sacrifice on their side.He said,  Everybody ain t down for that s**t, or whatever, but like I say, everybody has a different position of war.  He continued,  Because they don t give a f**k anyway.  He said again,  We might as well utilized them for that s**t and turn the tables on these n**ers. He said, that way  we can start lookin  like we ain t havin  that many casualties, and there can be more causalities on their side instead of ours. They are out their killing black people, black lives don t matter, that s what those mother f**kers   so we got to make it matter to them. Find a mother f**ker that is alone. Snap his ass, and then f***in hang him from a damn tree. Take a picture of it and then send it to the mother f**kers. We  just need one example,  and  then people will start watchin .  This will turn the tables on s**t, he said. He said this will start  a trickle-down effect.  He said that when one white person is hung and then they are just  flat-hanging,  that will start the  trickle-down effect.  He continued,  Black people are good at starting trends. He said that was how  to get the upper-hand. Another black man spoke up saying they needed to kill  cops that are killing us. The first black male said,  That will be the best method right there. Breitbart Texas previously reported how Sunshine was upset when  racist white people  infiltrated and disrupted one of her conference calls. She subsequently released the phone number of one of the infiltrators. The veteran immediately started receiving threatening calls.One of the #F***YoFlag movement supporters allegedly told a veteran who infiltrated their publicly posted conference call,  We are going to rape and gut your pregnant wife, and your f***ing piece of sh*t unborn creature will be hung from a tree. Breitbart Texas previously encountered Sunshine at a Sandra Bland protest at the Waller County Jail in Texas, where she said all white people should be killed. She told journalists and photographers,  You see this nappy-ass hair on my head?   That means I am one of those more militant Negroes.  She said she was at the protest because  these redneck mother-f**kers murdered Sandra Bland because she had nappy hair like me. #FYF911 black radicals say they will be holding the  imperial powers  that are actually responsible for the terrorist attacks on September 11th accountable on that day, as reported by Breitbart Texas. There are several websites and Twitter handles for the movement. Palmetto Star  describes himself as one of the head organizers. He said in a YouTube video that supporters will be burning their symbols of  the illusion of their superiority,  their  false white supremacy,  like the American flag, the British flag, police uniforms, and Ku Klux Klan hoods.Sierra McGrone or  Nocturnus Libertus  posted,  you too can help a young Afrikan clean their a** with the rag of oppression.  She posted two photos, one that appears to be herself, and a photo of a black man, wiping their naked butts with the American flag.For entire story: Breitbart News'\ninput ids: [101, 2053, 7615, 2003, 3517, 2013, 13857, 8112, 2372, 1997, 1996, 1001, 1042, 2100, 2546, 2683, 14526, 2030, 1001, 11865, 4801, 11253, 17802, 1998, 1001, 2304, 3669, 6961, 18900, 3334, 5750, 2170, 2005, 1996, 11404, 2075, 1998, 5689, 1997, 2317, 2111, 1998, 10558, 1012, 2027, 6628, 2500, 2006, 1037, 2557, 2265, 9857, 2305, 2000, 2735, 1996, 10401, 1998, 3102, 2317, 2111, 1998, 10558, 2000, 4604, 1037, 4471, 2055, 1996, 4288, 1997, 2304, 2111, 1999, 2637, 1012, 2028, 1997, 1996, 1042, 1008, 1008, 1008, 10930, 10258, 8490, 18829, 2003, 2170, 9609, 1012, 2016, 2038, 1037, 2557, 9927, 2265, 4354, 2013, 102]\nattention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define Model","metadata":{}},{"cell_type":"code","source":"# Define helper functions for encoding and constructing the dataset\ndef construct_encodings(x, tokenizer, max_len, trucation=True, padding=True):\n    return tokenizer(x, max_length=max_len, truncation=trucation, padding=padding)","metadata":{"execution":{"iopub.status.busy":"2024-10-20T04:16:12.661352Z","iopub.execute_input":"2024-10-20T04:16:12.661798Z","iopub.status.idle":"2024-10-20T04:16:12.668377Z","shell.execute_reply.started":"2024-10-20T04:16:12.661746Z","shell.execute_reply":"2024-10-20T04:16:12.667148Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"x = [str(item) for item in x]","metadata":{"execution":{"iopub.status.busy":"2024-10-20T04:16:12.669621Z","iopub.execute_input":"2024-10-20T04:16:12.669977Z","iopub.status.idle":"2024-10-20T04:16:12.696480Z","shell.execute_reply.started":"2024-10-20T04:16:12.669924Z","shell.execute_reply":"2024-10-20T04:16:12.695597Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"encodings = construct_encodings(x, tokenizer, max_len=MAX_LEN)\ninput_ids = encodings['input_ids']\nattention_masks = encodings['attention_mask']\n\n# Split the data into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\n\nx_train, x_test, y_train, y_test, masks_train, masks_test = train_test_split(\n    input_ids, y, attention_masks, test_size=TEST_SPLIT, random_state=42, stratify=y\n)\n\n# Apply SMOTE to the training data only\nsmote = SMOTE(random_state=42)\nx_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)\n\n# Adjust attention masks for the new samples generated by SMOTE\n# Note: SMOTE only generates synthetic samples based on input features. We'll copy masks for simplicity.\nmasks_train_resampled = np.array([masks_train[i] for i in smote.sample_indices_])\n\n# Reconstruct the encodings with resampled data\ntrain_encodings = {'input_ids': x_train_resampled, 'attention_mask': masks_train_resampled}\ntest_encodings = {'input_ids': x_test, 'attention_mask': masks_test}\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T04:16:12.697606Z","iopub.execute_input":"2024-10-20T04:16:12.697921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def construct_tfdataset(encodings, y=None):\n#     if y:\n#         return tf.data.Dataset.from_tensor_slices((dict(encodings),y))\n#     else:\n#         # this case is used when making predictions on unseen samples after training\n#         return tf.data.Dataset.from_tensor_slices(dict(encodings))\n    if y is not None:\n        return tf.data.Dataset.from_tensor_slices((dict(encodings), y))\n    else:\n        return tf.data.Dataset.from_tensor_slices(dict(encodings))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Changes:** I updated construct_tfdataset function.\n\n","metadata":{}},{"cell_type":"code","source":"# Create TensorFlow datasets\n# tfdataset = construct_tfdataset(encodings,y)\n\ntfdataset_train = construct_tfdataset(train_encodings, y_train_resampled).batch(BATCH_SIZE)\ntfdataset_test = construct_tfdataset(test_encodings, y_test).batch(BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train and evaluate","metadata":{}},{"cell_type":"code","source":"# TEST_SPLIT = 0.3\n# BATCH_SIZE = 16 \n\n# train_size = int(len(x) * (1-TEST_SPLIT))\n\n# tfdataset = tfdataset.shuffle(len(x))\n# tfdataset_train = tfdataset.take(train_size)\n# tfdataset_test = tfdataset.skip(train_size)\n\n# tfdataset_train = tfdataset_train.batch(BATCH_SIZE)\n# tfdataset_test = tfdataset_test.batch(BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Changes:** I changed nepochs from 5 to 16.\n\n* Increased batch size for better GPU utilization","metadata":{}},{"cell_type":"code","source":"# Define model parameters\nN_EPOCHS = 3\nDROPOUT_RATE = 0.3\n\n# Load the pre-trained DistilBERT base model (without classification head)\nbase_model = TFDistilBertModel.from_pretrained(MODEL_NAME)\n\n# Define input layers\ninput_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\nattention_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"attention_mask\")\n\n# Wrap the base model call in a Lambda layer to ensure compatibility\ndistilbert_output = Lambda(\n    lambda x: base_model(x[0], attention_mask=x[1])[0],  # Extract the last hidden state directly\n    output_shape=(MAX_LEN, base_model.config.hidden_size)\n)([input_ids, attention_mask])\n\n# Extract the [CLS] token's hidden state\nhidden_state = distilbert_output[:, 0, :]  # Extract the first token ([CLS]) representation\n\n# Apply dropout with a higher rate (e.g., 0.3 or 0.5)\ndropout = Dropout(DROPOUT_RATE)(hidden_state)\n\n# Output layer for classification (binary classification, so one Dense layer with 2 units)\noutput = Dense(2, activation='softmax')(dropout)\n\n# Create the new model\nmodel = Model(inputs=[input_ids, attention_mask], outputs=output)\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, clipnorm=1.0)\n# Compile model with the optimizer\nmodel.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n\n# 5. Print model summary to see the updated architecture\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Changes:** I added Gradient Clipping(clipnorm=1.0) and increased dropout rate to resolve overfitting\n\n* Gradient clipping ensures that updates to model weights don’t become too large, maintaining stability during training.\n* Track F1-score and precision/recall to ensure balanced performance. The classification report shows data imbalance so added this to monitor.\n* I added a Dropout layer with a rate of 0.3 after the DistilBERT output. This change aims to reduce overfitting by randomly deactivating 30% of neurons during training, which can help the model generalize better to new data.","metadata":{}},{"cell_type":"code","source":"# Calculate class weights\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(y),\n    y=y\n)\nclass_weights_dict = dict(enumerate(class_weights))\nprint(\"Class Weights:\", class_weights_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Changes:** I added class weights to address data imbalance.\n\n* Class weights help to give more importance to the minority class during training, which balances out the learning process.\n* This method is suitable because it integrates well with the model.fit() method of Keras without needing to change the structure of the dataset.","metadata":{}},{"cell_type":"code","source":"# # Define callbacks\n# # Train the model with early stopping\n# early_stopping = EarlyStopping(\n#     monitor='val_loss',  # Monitor the validation loss\n#     patience=3,  # Stop training if val_loss does not improve for 3 epochs\n#     restore_best_weights=True  # Restore model weights from the best epoch\n# )\n\n# # Ensure the directory exists for saving the model\n# os.makedirs('./model', exist_ok=True)\n\n# checkpoint = ModelCheckpoint(\n#     filepath='./model/best_model.weights.h5',  # Save the best model weights to this file\n#     monitor='val_loss',  # Monitor validation loss to determine the best model\n#     save_best_only=True,  # Only save the model if the validation loss improves\n#     save_weights_only=True,  # Only save the weights\n#     verbose=1  # Print messages when saving the model\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model with the callbacks\nhistory = model.fit(\n    tfdataset_train,\n    validation_data=tfdataset_test,  # Provide validation data to monitor val_loss\n    epochs=N_EPOCHS,\n    validation_freq=1,\n    class_weight=class_weights_dict,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Changes:** I added validation_freq=1\n* By default, model.fit() validates at the end of each epoch, but you can adjust this to validate more frequently to keep a closer eye on validation performance.\n","metadata":{}},{"cell_type":"code","source":"benchmarks = model.evaluate(tfdataset_test, return_dict=True, batch_size=BATCH_SIZE)\nprint(benchmarks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_predictor(model, model_name, max_len):\n  tkzr = DistilBertTokenizer.from_pretrained(model_name)\n  def predict_proba(text):\n      x = [text]\n\n      encodings = construct_encodings(x, tkzr, max_len=max_len)\n      tfdataset = construct_tfdataset(encodings)\n      tfdataset = tfdataset.batch(1)\n\n      preds = model.predict(tfdataset).logits\n      preds = activations.softmax(tf.convert_to_tensor(preds)).numpy()\n      return preds[0][0]\n    \n  return predict_proba\n\nclf = create_predictor(model, MODEL_NAME, MAX_LEN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The code below is what I added since the original code didn’t include any visualization. I referred to the second reference code (BERT model) since it had a lot of visualizations. I’ve added things like the confusion matrix and classification report.**","metadata":{}},{"cell_type":"code","source":"# 1. Plot Training/Validation Loss and Accuracy\ndef plot_metrics(history):\n    plt.figure(figsize=(12, 5))\n    # Plot accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.legend()\n\n    # Plot loss\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n\n    plt.show()\n\nplot_metrics(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2. Evaluate and Generate Predictions\ny_pred_probs = model.predict(tfdataset_test)\ny_pred = tf.argmax(y_pred_probs, axis=1).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get true labels\ny_true = []\nfor _, labels in tfdataset_test.unbatch():\n    y_true.append(labels.numpy())\ny_true = tf.convert_to_tensor(y_true).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3. Print Classification Report\nprint(\"Classification Report:\")\nprint(classification_report(y_true, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4. Plot Confusion Matrix\nconf_matrix = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Ensure the directory exists for saving the model\nos.makedirs('./model', exist_ok=True)\n# Save the Keras model with a proper extension.\nmodel.save('./model/clf.keras') # Use .keras or .h5 as the file extension.\n# Save additional information like model name and MAX_LEN using pickle.\nwith open('./model/info.pkl', 'wb') as f:\n    pickle.dump((MODEL_NAME, MAX_LEN), f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}